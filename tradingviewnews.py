import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timedelta, timezone
import time
import random
from github import Github
import base64
import re

# ============================================================
# è¨­å®šå€¼
# ============================================================
# Google API è¨­å®š
GOOGLE_API_KEY = ""  # å¡«å…¥ä½ çš„ API Key
GOOGLE_CX = ""  # å¡«å…¥ä½ çš„ Custom Search Engine ID

# GitHub è¨­å®š
GITHUB_TOKEN = ""  # å¡«å…¥ä½ çš„ GitHub Personal Access Token
GITHUB_REPO = ""  # å¡«å…¥æ ¼å¼ï¼šusername/repository_name
GITHUB_FILE_PATH = "reuters_archive.json"  # JSON æª”æ¡ˆåœ¨ GitHub çš„è·¯å¾‘

# çˆ¬èŸ²è¨­å®š
DEFAULT_DAYS = 5      # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼Œé è¨­æŠ“å–çš„å¤©æ•¸
MAX_SEARCH_RESULTS = 20 # æ¯å€‹é¡åˆ¥æœå°‹çš„çµæœæ•¸é‡
BASE_DELAY = 3        # åŸºç¤å»¶é²ç§’æ•¸
CATEGORY_DELAY = 5    # æ¯å€‹é¡åˆ¥è™•ç†å¾Œçš„å»¶é²ç§’æ•¸

# æœå°‹é¡åˆ¥é…ç½® (å·²ç°¡åŒ–ï¼Œæ‰€æœ‰é¡åˆ¥éƒ½ä½¿ç”¨çµ±ä¸€ç­–ç•¥)
SEARCH_CATEGORIES = [
    {
        "name": "è·¯é€æ—©å ±",
        "source": "TradingView - Reuters Morning Brief",
        "search_pattern": "ã€Šè·¯é€æ—©å ±ã€‹",
        "title_keywords": ["è·¯é€æ—©å ±"]
    },
    {
        "name": "å…¨çƒåŒ¯å¸‚",
        "source": "TradingView - Global FX",
        "search_pattern": "å…¨çƒåŒ¯å¸‚",
        "title_keywords": ["å…¨çƒåŒ¯å¸‚"]
    },
    {
        "name": "ç¾åœ‹å‚µå¸‚",
        "source": "TradingView - US Bonds",
        "search_pattern": "ç¾åœ‹å‚µå¸‚",
        "title_keywords": ["ç¾åœ‹å‚µå¸‚"]
    },
    {
        "name": "å°ç£åŒ¯å¸‚",
        "source": "TradingView - Taiwan FX",
        "search_pattern": "å°ç£åŒ¯å¸‚",
        "title_keywords": ["å°ç£åŒ¯å¸‚"]
    },
    {
        "name": "å°ç£å‚µå¸‚",
        "source": "TradingView - Taiwan Bonds",
        "search_pattern": "å°ç£å‚µå¸‚",
        "title_keywords": ["å°ç£å‚µå¸‚"]
    }
]

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
]

# ============================================================
# æª¢æŸ¥å¿…è¦è¨­å®š
# ============================================================
if not GOOGLE_API_KEY or not GOOGLE_CX:
    print("âŒ è«‹å¡«å…¥ GOOGLE_API_KEY å’Œ GOOGLE_CX")
    exit(1)

if not GITHUB_TOKEN or not GITHUB_REPO:
    print("âŒ è«‹å¡«å…¥ GITHUB_TOKEN å’Œ GITHUB_REPO")
    exit(1)

print("=" * 80)
print("TradingView è·¯é€æ–°èçˆ¬èŸ² - çµ‚æ¥µç°¡åŒ–ç‰ˆ")
print("=" * 80)

# ============================================================
# é€£æ¥ GitHub
# ============================================================
print("\næ­£åœ¨é€£æ¥ GitHub...")
try:
    g = Github(GITHUB_TOKEN)
    repo = g.get_repo(GITHUB_REPO)
    print(f"âœ“ å·²é€£æ¥åˆ°å„²å­˜åº«ï¼š{GITHUB_REPO}")
except Exception as e:
    print(f"âŒ GitHub é€£æ¥å¤±æ•—ï¼š{e}")
    exit(1)

# ============================================================
# å¾ GitHub è®€å–ç¾æœ‰è³‡æ–™
# ============================================================
print("\næ­£åœ¨è®€å– GitHub ä¸Šçš„å­˜æª”è³‡æ–™...")
existing_data = {"last_updated": "", "reports": []}
file_sha = None
existing_titles = set()

try:
    file_content = repo.get_contents(GITHUB_FILE_PATH)
    file_sha = file_content.sha
    content_text = base64.b64decode(file_content.content).decode('utf-8')
    existing_data = json.loads(content_text)
    # å»ºç«‹ç¾æœ‰æ¨™é¡Œçš„é›†åˆï¼Œç”¨æ–¼å¿«é€Ÿæ¯”å°
    existing_titles = {report.get('Title') for report in existing_data.get('reports', [])}
    print(f"âœ“ æ‰¾åˆ°å­˜æª”æª”æ¡ˆï¼Œç¾æœ‰å ±å°æ•¸ï¼š{len(existing_data.get('reports', []))} ç¯‡")
except Exception as e:
    print(f"âš ï¸  è®€å–å¤±æ•—ï¼ˆå¯èƒ½æ˜¯ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼‰ï¼Œå°‡å»ºç«‹æ–°æª”æ¡ˆã€‚")

# ============================================================
# è¨ˆç®—éœ€è¦æŠ“å–çš„æ—¥æœŸç¯„åœ
# ============================================================
today = datetime.now(timezone(timedelta(hours=8)))
last_updated_str = existing_data.get('last_updated', '')
days_to_scrape = DEFAULT_DAYS

if last_updated_str:
    try:
        last_date = datetime.strptime(last_updated_str, '%Y-%m-%d').replace(tzinfo=timezone(timedelta(hours=8)))
        days_diff = 20#(today - last_date).days
        if days_diff <= 0:
            print(f"\nâœ“ è³‡æ–™å·²æ˜¯æœ€æ–°ï¼ˆæœ€å¾Œæ›´æ–°ï¼š{last_updated_str}ï¼‰ã€‚")
            exit(0)
        days_to_scrape = days_diff if days_diff > 0 else 1
    except ValueError:
        print(f"âš ï¸ ç„¡æ³•è§£æä¸Šæ¬¡æ›´æ–°æ—¥æœŸï¼Œä½¿ç”¨é è¨­å¤©æ•¸: {DEFAULT_DAYS} å¤©")

print(f"\nå°‡æœå°‹éå» {days_to_scrape} å¤©å…§çš„æ–°èã€‚")
print("=" * 80)

# ============================================================
# è¼”åŠ©å‡½æ•¸
# ============================================================
def extract_datetime_from_page(soup):
    """å¾é é¢ <time> æ¨™ç±¤æå–æº–ç¢ºçš„ datetime ç‰©ä»¶ (å·²è½‰æ›ç‚º UTC+8)"""
    time_tag = soup.find('time', datetime=True)
    if time_tag and time_tag.get('datetime'):
        try:
            datetime_str = time_tag['datetime']
            dt_utc = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
            dt_taipei = dt_utc.astimezone(timezone(timedelta(hours=8)))
            return dt_taipei
        except Exception:
            return None
    return None

def fetch_page_content(url):
    """æŠ“å–å–®ä¸€é é¢çš„å…§å®¹ä¸¦è§£æ"""
    time.sleep(BASE_DELAY + random.uniform(0, 2))
    headers = {'User-Agent': random.choice(USER_AGENTS), 'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8'}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        h1_title = soup.find('h1', class_=re.compile(r'title-'))
        page_title = h1_title.get_text(strip=True) if h1_title else "æ¨™é¡Œç„¡æ³•å–å¾—"

        content_div = soup.find('div', class_=re.compile(r'content-')) or soup.find('article')
        content = ""
        if content_div:
            for tag in content_div(['script', 'style', 'aside']):
                tag.decompose()
            content = content_div.get_text(separator='\n', strip=True)

        return page_title, content, soup
    except Exception as e:
        print(f"  â”” éŒ¯èª¤ï¼šæŠ“å–é é¢ {url} å¤±æ•— - {e}")
        return None, None, None

# ============================================================
# ä¸»è¦çˆ¬å–é‚è¼¯
# ============================================================
all_new_reports = []
api_call_count = 0

print("\nã€é–‹å§‹åŸ·è¡Œæœå°‹ä»»å‹™ã€‘")
for category_index, category in enumerate(SEARCH_CATEGORIES, 1):
    print(f"\n--- [{category_index}/{len(SEARCH_CATEGORIES)}] è™•ç†é¡åˆ¥ï¼š{category['name']} ---")
    query = f"site:tw.tradingview.com/news/reuters.com/ \"{category['search_pattern']}\""

    print(f"  æœå°‹ï¼š{query}")
    api_url = "https://www.googleapis.com/customsearch/v1"
    params = {
        'key': GOOGLE_API_KEY, 'cx': GOOGLE_CX, 'q': query,
        'num': MAX_SEARCH_RESULTS, 'lr': 'lang_zh-TW',
        'sort': 'date', 'dateRestrict': f'd{days_to_scrape}'
    }

    try:
        api_call_count += 1
        print(f"  æ­£åœ¨å‘¼å« Google API... (ä»Šæ—¥ç¬¬ {api_call_count} æ¬¡)")
        time.sleep(CATEGORY_DELAY + random.uniform(0, 3))
        response = requests.get(api_url, params=params, timeout=20)
        response.raise_for_status()
        search_results = response.json().get('items', [])
        print(f"  âœ“ æ‰¾åˆ° {len(search_results)} å€‹çµæœ")

        for item in search_results:
            url = item.get('link', '')
            google_title = item.get('title', '')
            print(f"\n  > æª¢æŸ¥: {google_title[:50]}...")

            if 'tw.tradingview.com/news/reuters.com' not in url:
                print("  â”” ç•¥éï¼šéç›®æ¨™æ–°èé€£çµ")
                continue

            page_title, content, soup = fetch_page_content(url)
            if not all([page_title, content, soup]) or page_title == "æ¨™é¡Œç„¡æ³•å–å¾—":
                continue

            # **æ ¸å¿ƒé‚è¼¯ï¼šç›´æ¥æ¯”å°æ¨™é¡Œæ˜¯å¦å·²å­˜åœ¨**
            if page_title in existing_titles:
                print("  â”” ç•¥éï¼šå ±å°æ¨™é¡Œå·²å­˜åœ¨æ–¼ GitHub å­˜æª”")
                continue

            if not any(keyword in page_title for keyword in category['title_keywords']):
                print(f"  â”” ç•¥éï¼šæ¨™é¡Œä¸å«é—œéµå­— {category['title_keywords']}")
                continue

            if len(content) < 100:
                print(f"  â”” ç•¥éï¼šå…§å®¹éçŸ­ ({len(content)} å­—å…ƒ)")
                continue

            article_datetime = extract_datetime_from_page(soup)
            if not article_datetime:
                print("  â”” è­¦å‘Šï¼šç„¡æ³•å¾é é¢æå–æº–ç¢ºæ—¥æœŸï¼Œå°‡ä½¿ç”¨ç•¶å‰æ—¥æœŸ")
                article_datetime = today

            report_data = {
                "Source": category['source'], "URL": url,
                "Date": article_datetime.strftime('%Y-%m-%d'),
                "Time": article_datetime.strftime('%H:%M:%S'),
                "Title": page_title, "Content": content, "ContentLength": len(content),
                "ScrapedAt": today.strftime('%Y-%m-%d %H:%M:%S')
            }
            all_new_reports.append(report_data)
            existing_titles.add(page_title) # æ–°å¢è‡³é›†åˆï¼Œé¿å…æœ¬æ¬¡åŸ·è¡Œé‡è¤‡æŠ“å–
            print(f"  â˜…â˜…â˜…â˜…â˜… æˆåŠŸåŠ å…¥æ–°å ±å°ï¼â˜…â˜…â˜…â˜…â˜…")

    except Exception as e:
        print(f"âŒ è™•ç†é¡åˆ¥ {category['name']} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

# ============================================================
# çµæœå»é‡èˆ‡çµ±è¨ˆ
# ============================================================
print("\n" + "=" * 80)
print("æŠ“å–å®Œæˆï¼Œé€²è¡Œæœ€çµ‚è™•ç†...")
print("=" * 80)
print(f"API ç¸½å‘¼å«æ¬¡æ•¸ï¼š{api_call_count}")

# æœ€çµ‚å»é‡ (ä»¥é˜²è¬ä¸€)
unique_reports = []
seen_titles_final = set()
for report in all_new_reports:
    if report['Title'] not in seen_titles_final:
        unique_reports.append(report)
        seen_titles_final.add(report['Title'])

print(f"\nâœ“ æœ¬æ¬¡å…±æ–°å¢ï¼š{len(unique_reports)} ç¯‡")
if unique_reports:
    category_stats = {}
    for report in unique_reports:
        source = report['Source']
        category_stats[source] = category_stats.get(source, 0) + 1
    print("\nå„é¡åˆ¥çµ±è¨ˆï¼š")
    for source, count in sorted(category_stats.items()):
        print(f"  {source}: {count} ç¯‡")

# ============================================================
# æ›´æ–° GitHub è³‡æ–™
# ============================================================
if unique_reports:
    print("\n" + "=" * 80)
    print("æ­£åœ¨æ›´æ–° GitHub è³‡æ–™...")

    final_reports = existing_data.get('reports', []) + unique_reports
    final_reports.sort(key=lambda x: (x.get('Date', ''), x.get('Time', '')), reverse=True)

    updated_data = {
        "last_updated": today.strftime('%Y-%m-%d'),
        "total_reports": len(final_reports),
        "reports": final_reports
    }
    json_content = json.dumps(updated_data, ensure_ascii=False, indent=2)

    try:
        commit_message = f"æ–°èæ›´æ–° {today.strftime('%Y-%m-%d')}ï¼šæ–°å¢ {len(unique_reports)} ç¯‡"
        if file_sha:
            repo.update_file(GITHUB_FILE_PATH, commit_message, json_content, file_sha)
            print(f"âœ“ å·²æ›´æ–° GitHub æª”æ¡ˆ")
        else:
            repo.create_file(GITHUB_FILE_PATH, commit_message, json_content)
            print(f"âœ“ å·²å»ºç«‹ GitHub æª”æ¡ˆ")

        print("\nğŸ“‹ æœ¬æ¬¡æ–°å¢çš„å ±å°æ‘˜è¦ï¼š")
        for i, r in enumerate(sorted(unique_reports, key=lambda x: (x['Date'], x['Time'])), 1):
            print(f"{i}. [{r['Date']} {r['Time']}] {r['Title'][:50]}... ({r['Source']})")

    except Exception as e:
        print(f"âŒ GitHub æ›´æ–°å¤±æ•—ï¼š{e}")
        backup_filename = f"reuters_backup_{today.strftime('%Y%m%d_%H%M%S')}.json"
        with open(backup_filename, 'w', encoding='utf-8') as f:
            f.write(json_content)
        print(f"âœ“ å·²å„²å­˜æœ¬åœ°å‚™ä»½ï¼š{backup_filename}")
else:
    print("\nâš ï¸  æœ¬æ¬¡æ²’æœ‰æ–°å¢ä»»ä½•å ±å°ï¼Œä¸æ›´æ–° GitHubã€‚")

print("\n" + "=" * 80)
print("ç¨‹å¼åŸ·è¡Œå®Œç•¢ï¼")
print("=" * 80)

